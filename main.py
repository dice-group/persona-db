import os
import json
import time
from datasets import Dataset
from config import (
    MODEL_NAME,
    DATASET_NAME,
    DATASET_SUBSET,
    DATASET_SPLIT,
    TEMPLATE_PATH,
    RESULTS_DIR,
    MAX_NEW_TOKENS,
    TEMPERATURE,
    MAX_NUM_SEQS,
)
from utils import (
    load_json_template,
    ensure_directory_exists,
    extract_json_from_output,
    get_processed_persona_ids,
)
from prompt_builder import build_extraction_prompt
from vllm import LLM, SamplingParams
from tqdm import tqdm
from typing import List, Dict, Any, Tuple


def main() -> None:
    """
    Main function to process personas using a language model, generate outputs, 
    extract JSON, and save results to disk.

    Loads the dataset, builds prompts, generates outputs with vLLM, extracts JSON,
    and saves results or error logs for each persona.
    """
    print(f"Initializing vLLM with model: {MODEL_NAME}...")
    llm = LLM(
        MODEL_NAME,
        dtype="auto",
        max_num_seqs=MAX_NUM_SEQS,
        max_model_len=131072,
        trust_remote_code=True,
        tensor_parallel_size=4,
        gpu_memory_utilization=0.9,
    )
    print("vLLM model initialized.")

    sampling_params = SamplingParams(temperature=TEMPERATURE, max_tokens=MAX_NEW_TOKENS)
    print(
        f"Sampling parameters: Temperature={TEMPERATURE}, Max New Tokens={MAX_NEW_TOKENS}"
    )

    print(f"Loading dataset: {DATASET_NAME}/{DATASET_SUBSET} split {DATASET_SPLIT}...")
    full_datasets = load_dataset(DATASET_NAME, DATASET_SUBSET, split=DATASET_SPLIT)
    total_personas_in_dataset = len(full_datasets)
    print(f"Dataset loaded. Total personas: {total_personas_in_dataset}")

    print(f"Loading JSON template from: {TEMPLATE_PATH}...")
    template_json = load_json_template(TEMPLATE_PATH)
    print("Template loaded successfully.")

    ensure_directory_exists(RESULTS_DIR)
    print(f"Ensured results directory exists: {RESULTS_DIR}")

    processed_ids_at_start = get_processed_persona_ids(RESULTS_DIR)
    initial_completed_count = len(processed_ids_at_start)

    personas_to_process_tuples: List[Tuple[int, str]] = []
    for global_idx, data_entry in enumerate(full_datasets):
        if global_idx not in processed_ids_at_start:
            personas_to_process_tuples.append((global_idx, data_entry["persona"]))

    remaining_at_start_of_run = len(personas_to_process_tuples)

    print(
        f"\n--- Progress Summary: {initial_completed_count} successfully processed, {remaining_at_start_of_run} to process in this run (out of {total_personas_in_dataset} total) ---\n"
    )

    if remaining_at_start_of_run == 0:
        print("All personas have been successfully processed. Exiting.")
        return

    list_of_prompts_for_vllm: List[str] = []
    vllm_output_idx_to_global_idx: Dict[int, int] = {}
    for i, (global_idx, persona_text) in enumerate(personas_to_process_tuples):
        prompt = build_extraction_prompt(persona_text, template_json)
        list_of_prompts_for_vllm.append(prompt)
        vllm_output_idx_to_global_idx[i] = global_idx

    print(f"Sending {len(list_of_prompts_for_vllm)} prompts to vLLM for generation...")

    start_vllm_generation_time = time.time()
    request_outputs = llm.generate(
        prompts=list_of_prompts_for_vllm,
        sampling_params=sampling_params,
    )
    end_vllm_generation_time = time.time()
    vllm_total_inference_time = end_vllm_generation_time - start_vllm_generation_time
    print(
        f"vLLM generation completed for {len(list_of_prompts_for_vllm)} prompts in {vllm_total_inference_time:.2f} seconds."
    )
    if len(list_of_prompts_for_vllm) > 0:
        print(
            f"Average vLLM inference time per persona: {vllm_total_inference_time / len(list_of_prompts_for_vllm):.2f} seconds."
        )

    total_saving_and_processing_time = 0
    completed_in_this_run_counter = 0

    print("\nStarting to process and save generated outputs...")
    with tqdm(
        total=remaining_at_start_of_run,
        desc="Saving & Processing Outputs",
        unit="persona",
    ) as pbar:
        for i, output in enumerate(request_outputs):
            start_time_persona_save = time.time()

            global_idx = vllm_output_idx_to_global_idx[i]

            if not output.outputs:
                generated_text = ""
                error_message = "No output generated by vLLM."
                print(
                    f"\n!!! WARNING: No output generated for persona {global_idx}. Error: {error_message}"
                )
                filename = os.path.join(
                    RESULTS_DIR, f"persona_{global_idx}_error_no_output.txt"
                )
                with open(filename, "w", encoding="utf-8") as f:
                    f.write(
                        f"No output generated for persona {global_idx}. Original prompt:\n{list_of_prompts_for_vllm[i]}\nError: {error_message}"
                    )

                completed_in_this_run_counter += 1
                pbar.update(1)
                time_taken_persona_save = time.time() - start_time_persona_save
                total_saving_and_processing_time += time_taken_persona_save
                current_absolute_remaining = total_personas_in_dataset - (
                    initial_completed_count + completed_in_this_run_counter
                )
                pbar.set_postfix_str(
                    f"Save Time: {time_taken_persona_save:.2f}s | Abs Remaining: {current_absolute_remaining}"
                )
                continue

            generated_text = output.outputs[0].text

            extracted_json_str = extract_json_from_output(generated_text)

            try:
                parsed_json = json.loads(extracted_json_str)

                filename = os.path.join(RESULTS_DIR, f"persona_{global_idx}.json")
                with open(filename, "w", encoding="utf-8") as f:
                    json.dump(parsed_json, f, indent=2, ensure_ascii=False)

                completed_in_this_run_counter += 1
                pbar.update(1)

            except json.JSONDecodeError as e:
                print(
                    f"\n!!! ERROR: Could not extract valid JSON for persona {global_idx}. JSONDecodeError: {e}"
                )
                print(
                    f"!!! Problematic string (first 500 chars): \n{extracted_json_str[:500]}..."
                )
                print("!!! Saving full raw output to a .txt file for debugging.")
                filename = os.path.join(RESULTS_DIR, f"persona_{global_idx}_error.txt")
                with open(filename, "w", encoding="utf-8") as f:
                    f.write(generated_text)

                completed_in_this_run_counter += 1
                pbar.update(1)
            except Exception as e:
                print(
                    f"\n!!! An unexpected error occurred for persona {global_idx} during saving: {e}"
                )
                filename = os.path.join(RESULTS_DIR, f"persona_{global_idx}_error.txt")
                with open(filename, "w", encoding="utf-8") as f:
                    f.write(generated_text)

                completed_in_this_run_counter += 1
                pbar.update(1)

            end_time_persona_save = time.time()
            time_taken_persona_save = end_time_persona_save - start_time_persona_save
            total_saving_and_processing_time += time_taken_persona_save

            current_absolute_remaining = total_personas_in_dataset - (
                initial_completed_count + completed_in_this_run_counter
            )
            pbar.set_postfix_str(
                f"Save Time: {time_taken_persona_save:.2f}s | Abs Remaining: {current_absolute_remaining}"
            )

    print(f"\n--- All remaining personas processed ---")
    if remaining_at_start_of_run > 0:
        print(
            f"Total time for saving and post-processing {remaining_at_start_of_run} attempted personas: {total_saving_and_processing_time:.2f} seconds"
        )
        print(
            f"Average saving and post-processing time per persona: {total_saving_and_processing_time / remaining_at_start_of_run:.2f} seconds"
        )
    else:
        print("No new personas were processed in this run.")

    final_successful_count = len(get_processed_persona_ids(RESULTS_DIR))
    print(
        f"Total successfully processed JSONs: {final_successful_count} out of {total_personas_in_dataset}"
    )


if __name__ == "__main__":
    print("\n--- Starting Persona Processing with vLLM ---")
    main()
